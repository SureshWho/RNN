{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SureshWho/RNN/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h4QKlfrAUSEy"
   },
   "source": [
    "\n",
    "# RNN\n",
    "\n",
    "## Introduction\n",
    "Recurrent Neural Network **(RNN)**, used for processing sequence of data, unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. \n",
    "\n",
    "- Input X, Output, Y, or both could be a sequence.\n",
    "- X and Y could have same or different in lengths.\n",
    "\n",
    "### Why not standard Neural Network?\n",
    "- Sequence might have different Inputs, outputs lengths for each example.\n",
    "- Does not share features learnt across different positions (of the text  or symbol). \n",
    "- Does not learn the relationship between different symbols (words) in the sequence.\n",
    "- Might become very huge number of input parameters (10000 (words/symbols) * sequence length)\n",
    "\n",
    "## Notations\n",
    "\n",
    "- **$x<t>$** - Discrete value from the input sequence *x* at time *t*. Eg. $x$<2> represents $2^{nd}$ value from input sequence x.\n",
    "- **$y<t>$** - $t^{th}$ sequence from output y. Eg. $y$<2> represents $2^{nd}$ value from output sequence y.\n",
    "- $T_{x}$ - Number of sequence in input $x$.\n",
    "- $T_{y}$ - Number of sequence in input $x$.\n",
    "- $X^{(i)<t>}$ - Referes to $t^{th}$ element in input sequence X for $i^{th}$ example in the training set.\n",
    "- $T^{(i)}_{x}$ - Referes to input sequence length for $i^{th}$ example.\n",
    "- $y^{(i)<t>}$ - Referes to $t^{th}$ element in output sequence y for $i^{th}$ example in the training set.\n",
    "- $T^{(i)}_{y}$ - Referes to output sequence length for $i^{th}$ example.\n",
    "- **$x<t>$** - TODO Represented as one-shot encoding from the dictionary of possible sequence element.\n",
    "\n",
    "\n",
    "## Tips\n",
    "- If a word/sequence type is not in dictionary represent it as \"UNKNOWN\".\n",
    "- Activation to first block in RNN generally initialized to zeros'. \n",
    "\n",
    "# RNN Network\n",
    "\n",
    "\n",
    "<img src=\"./images/rnn.png\">\n",
    "\n",
    "$a^{<t>} = g (W_{aa}  a^{<t-1>} + W_{ax} x^{<t>} + b_a)$. g -> Tanh\n",
    "\n",
    "$\\hat y^{<t>} = g (W_{ya} a^{<t>} + b_y)$. g -> Sigmoid / Softmax\n",
    "\n",
    "$a(0) = \\vec 0$\n",
    "\n",
    "## Dimensions\n",
    "\n",
    "$n_{a} = $ Vector size for a.\n",
    "\n",
    "$n_{x} = $ Vector size for x.\n",
    "\n",
    "$n_{y} = $ Vector size for y.\n",
    "\n",
    "$W_{aa} = (n_a, n_a) $\n",
    "\n",
    "$W_{ax} = (n_a, n_x) $\n",
    "\n",
    "$W_{ya} = (n_y, n_a) $\n",
    "\n",
    "$a^{<t>} = g (W_{aa}  a^{<t-1>} + W_{ax} x^{<t>} + b_a)$\n",
    "\n",
    "$(n_a, m) = g ( (n_a, n_a) (n_a, m) + (n_a, n_x) (n_x, m) + n_a )$\n",
    "\n",
    "$x^{<t>} = (n_x, m)$\n",
    "\n",
    "$a^{<t>} = (n_a, m)$\n",
    "\n",
    "## Weights Simplification \n",
    "\n",
    "$a^{T} = 100, W_{aa} = (100, 100)$\n",
    "\n",
    "$X^{<t>} = 10000, W_{ax} = (100, 10000)$\n",
    "\n",
    "$a^{<t>} = g (W_{aa}  a^{<t-1>} + W_{ax} x^{<t>} + b_a)$\n",
    "\n",
    "***$a^{<t>} = g (W_{a}[a^{<t-1>} ,  x^{<t>}] + b_a)$***\n",
    "\n",
    "$W_a = \\begin{bmatrix} \n",
    "W_{aa}\n",
    "|\n",
    "W_{ax}\n",
    "\\end{bmatrix} $ = (100, 10100)  \n",
    "\n",
    "$[ a^{<t-1>} , x^{<t>} ]= \\begin{bmatrix} \n",
    "a^{<t-1>} \\\\\n",
    "x^{<t>}\n",
    "\\end{bmatrix} $ = (10100,1 )  \n",
    "\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "W_{aa}\n",
    "|\n",
    "W_{ax}\n",
    "\\end{bmatrix} \\begin{bmatrix} \n",
    "a^{<t-1>} \\\\\n",
    "x^{<t>}\n",
    "\\end{bmatrix} $ = $a^{<t>} = W_{aa}  a^{<t-1>} + W_{ax} x^{<t>} $\n",
    "\n",
    "$a^{<t>} = g (W_{a}[a^{<t-1>} ,  x^{<t>}] + b_a)$\n",
    "\n",
    "$\\hat y^{<t>} = g (W_{y} a^{<t>} + b_y)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3V1PtTo-wVO"
   },
   "source": [
    "# Back Propagation\n",
    "\n",
    "<img src=\"./images/rnn_fbp.png\">\n",
    "\n",
    "Loss function for each sequence is defined as:\n",
    "\n",
    "$L(\\hat y^{<t>}, y^{<t>}) = -y^{<t>}log (\\hat y^{<t>}) -(1-y)^{<t>}log (1-\\hat y^{<t>})$\n",
    "\n",
    "Cost function is defined as :\n",
    "\n",
    "$L(\\hat y, y) = \\sum_{t=1}^{T_x} L(\\hat y^{<t>}, y^{<t>})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_yiUbzlaYXHk"
   },
   "source": [
    "# RNN Types\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n",
    "Standard RNN has issue like vanishing gradient for long sequence, Gated Recurrent Unit (GRU) can solove the issue.\n",
    "\n",
    "$\\tilde  C^{<t>}$ Candidate for updating \n",
    "\n",
    "$\\tilde  C^{<t>} = tanh (W_{c}[c^{<t-1>} ,  x^{<t>}] + b_c)$\n",
    "\n",
    "$\\Gamma_u^{<t>} = \\sigma (W_{u}[c^{<t-1>} ,  x^{<t>}] + b_u)$\n",
    "\n",
    "$C^{<t>} = \\Gamma_u^{<t>} * \\tilde  C^{<t>} + (1 - \\Gamma_u^{<t>}) * C^{<t-1>}$\n",
    "\n",
    "Full GRU\n",
    "\n",
    "$\\tilde  C^{<t>} = tanh (W_{c}[ \\Gamma_r^{<t-1>} * c^{<t-1>} ,  x^{<t>}] + b_c)$\n",
    "\n",
    "$\\Gamma_r^{<t>} = \\sigma (W_{r}[c^{<t-1>} ,  x^{<t>}] + b_r)$\n",
    "\n",
    "$\\Gamma_u^{<t>} = \\sigma (W_{u}[c^{<t-1>} ,  x^{<t>}] + b_u)$\n",
    "\n",
    "$C^{<t>} = \\Gamma_u^{<t>} * \\tilde  C^{<t>} + (1 - \\Gamma_u^{<t>}) * C^{<t-1>}$\n",
    "\n",
    "$a^{<t>} = c^{<t>}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "$\\tilde  C^{<t>} = tanh (W_{c}[a^{<t-1>} ,  x^{<t>}] + b_c)$\n",
    "\n",
    "***Update*** - $\\Gamma_u^{<t>} = \\sigma (W_{u}[a^{<t-1>} ,  x^{<t>}] + b_u$\n",
    "\n",
    "***Forget*** - $\\Gamma_f^{<t>} = \\sigma (W_{f}[a^{<t-1>} ,  x^{<t>}] + b_f$\n",
    "\n",
    "***Output*** - $\\Gamma_o^{<t>} = \\sigma (W_{o}[a^{<t-1>} ,  x^{<t>}] + b_o$\n",
    "\n",
    "$C^{<t>} = \\Gamma_u^{<t>} * \\tilde  C^{<t>} + \\Gamma_f^{<t>} * C^{<t-1>}$\n",
    "\n",
    "$a^{<t>} = \\Gamma_o^{<t>} * tanh c^{<t>}$\n",
    "\n",
    "***Peephole connection :*** add $C^{<t-1>}$ to all $\\Gamma$ gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "RNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
