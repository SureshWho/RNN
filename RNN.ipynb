{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SureshWho/RNN/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4QKlfrAUSEy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# RNN\n",
        "\n",
        "## Introduction\n",
        "Recurrent Neural Network **(RNN)**, used for processing sequence of data, unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. \n",
        "\n",
        "- Input X, Output, Y, or both could be sequence, not a discrete values.\n",
        "- X and Y could have same or different in lengths.\n",
        "\n",
        "### Why not standard Neural Network?\n",
        "- Sequence might have different Inputs, outputs lengths for each examples.\n",
        "- Does not share features learnt across different positions (of the text  or symbol). \n",
        "- Does not learn the relationship between different symbols (words) in the sequence.\n",
        "- Might become very huge number of input parameters (10000 (words/symbols) * sequence length)\n",
        "\n",
        "## Notations\n",
        "\n",
        "- **$x<t>$** - Discrete value from the input sequence *x* at time *t*. Eg. $x$<2> represents $2^{nd}$ value from input sequence x.\n",
        "- **$y<t>$** - $t^{th}$ sequence from output y. Eg. $y$<2> represents $2^{nd}$ value from output sequence y.\n",
        "- $T_{x}$ - Number of sequence in input $x$.\n",
        "- $T_{y}$ - Number of sequence in input $x$.\n",
        "- $X^{(i)<t>}$ - Referes to $t^{th}$ element in input sequence X for $i^{th}$ example in the training set.\n",
        "- $T^{(i)}_{x}$ - Referes to input sequence length for $i^{th}$ example.\n",
        "- $y^{(i)<t>}$ - Referes to $t^{th}$ element in output sequence y for $i^{th}$ example in the training set.\n",
        "- $T^{(i)}_{y}$ - Referes to output sequence length for $i^{th}$ example.\n",
        "- **$x<t>$** - TODO Represented as one-shot encoding from the dictionary of possible sequence element.\n",
        "\n",
        "## Tips\n",
        "- If a word/sequence type is not in dictionary represent it as \"UNKNOWN\".\n",
        "- Activation to first block in RNN generally initialized to zeros'. \n",
        "\n",
        "# RNN Network\n",
        "\n",
        "<img src=\"https://github.com/SureshWho/RNN/blob/master/images/rnn.png?raw=1\" style=\"width:750px;height:200px;\">\n",
        "\n",
        "$a^{<t>} = g (W_{aa}  a^{<t-1>} + W_{ax} x^{<t>} + b_a)$. g -> Tanh\n",
        "\n",
        "$\\hat y^{<t>} = g (W_{ya} a^{<t>} + b_y)$. g -> Sigmoid / Softmax\n",
        "\n",
        "$a(0) = \\vec 0$\n",
        "\n",
        "## Weights Simplification \n",
        "\n",
        "$a^{T} = 100, W_{aa} = (100, 100)$\n",
        "\n",
        "$X^{<t>} = 10000, W_{ax} = (100, 10000)$\n",
        "\n",
        "$a^{<t>} = g (W_{aa}  a^{<t-1>} + W_{ax} x^{<t>} + b_a)$\n",
        "\n",
        "***$a^{<t>} = g (W_{a}[a^{<t-1>} ,  x^{<t>}] + b_a)$***\n",
        "\n",
        "$W_a = \\begin{bmatrix} \n",
        "W_{aa}\n",
        "|\n",
        "W_{ax}\n",
        "\\end{bmatrix} $ = (100, 10100)  \n",
        "\n",
        "$[ a^{<t-1>} , x^{<t>} ]= \\begin{bmatrix} \n",
        "a^{<t-1>} \\\\\n",
        "x^{<t>}\n",
        "\\end{bmatrix} $ = (10100,1 )  \n",
        "\n",
        "\n",
        "$\\begin{bmatrix} \n",
        "W_{aa}\n",
        "|\n",
        "W_{ax}\n",
        "\\end{bmatrix} \\begin{bmatrix} \n",
        "a^{<t-1>} \\\\\n",
        "x^{<t>}\n",
        "\\end{bmatrix} $ = $a^{<t>} = W_{aa}  a^{<t-1>} + W_{ax} x^{<t>} $\n",
        "\n",
        "$a^{<t>} = g (W_{a}[a^{<t-1>} ,  x^{<t>}] + b_a)$\n",
        "\n",
        "$\\hat y^{<t>} = g (W_{y} a^{<t>} + b_y)$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3V1PtTo-wVO",
        "colab_type": "text"
      },
      "source": [
        "# Back Propagation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yiUbzlaYXHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}